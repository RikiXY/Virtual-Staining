{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizioni iniziali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pacchetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=None):\n",
    "    img = image\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    figure, axes = plt.subplots(figsize=(20, 20))\n",
    "    axes.imshow(img, cmap='gray')\n",
    "    axes.set_title(title)\n",
    "    axes.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_horizontal_images(images):\n",
    "    figure, subplots = plt.subplots(1, len(images), figsize=(20, 20))\n",
    "    figure.subplots_adjust(wspace=0.01)\n",
    "    for i, img in enumerate(images):\n",
    "        subplots[i].imshow(img[0], cmap='gray')\n",
    "        subplots[i].set_title(img[1])\n",
    "        subplots[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allineamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funzioni di misura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(img1: np.ndarray, img2: np.ndarray, bins: int = 256) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mutual information (MI) between two grayscale images.\n",
    "    \n",
    "    :param img1: First image in grayscale.\n",
    "    :param img2: Second image in grayscale.\n",
    "    :param bins: Number of histogram bins to use (256 for 8-bit images).\n",
    "    :return: Mutual information (in bits) between img1 and img2.\n",
    "    \"\"\"\n",
    "    # Convert images to 1D numpy arrays (flatten) if they are not already\n",
    "    img1 = img1.ravel()\n",
    "    img2 = img2.ravel()\n",
    "\n",
    "    # Compute the joint histogram\n",
    "    # histogram2d returns the 2D histogram and bin edges\n",
    "    joint_hist, x_edges, y_edges = np.histogram2d(img1, img2, bins=bins)\n",
    "    \n",
    "    # Normalize the joint histogram to get the joint PDF p(x,y)\n",
    "    joint_pdf = joint_hist / np.sum(joint_hist)\n",
    "    \n",
    "    # Compute the marginal PDFs p(x) and p(y)\n",
    "    p_x = np.sum(joint_pdf, axis=1)  # Sum over columns -> marginal over x\n",
    "    p_y = np.sum(joint_pdf, axis=0)  # Sum over rows -> marginal over y\n",
    "\n",
    "    # Only consider non-zero values to avoid log(0)\n",
    "    # MI = sum p(x,y)*log( p(x,y)/(p(x)*p(y)) )\n",
    "    non_zero_idxs = joint_pdf > 0\n",
    "    mi = np.sum(joint_pdf[non_zero_idxs] * \n",
    "                np.log2(joint_pdf[non_zero_idxs] / \n",
    "                        (p_x[np.newaxis].T @ p_y[np.newaxis])[non_zero_idxs]))\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def calculate_mse(img1, img2) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean squared error (MSE) between two images.\n",
    "    \n",
    "    :param img1: First image.\n",
    "    :param img2: Second image.\n",
    "    :return: Mean squared error between img1 and img2.\n",
    "    \"\"\"\n",
    "    # Compute the squared error between the two images\n",
    "    se = (img1 - img2) ** 2\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = np.mean(se)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_DISTANCE = 20\n",
    "LOWES_RATIO = 0.75\n",
    "\n",
    "def filter_LR(matches, threshold=LOWES_RATIO):\n",
    "    filtered_matches = []\n",
    "    for m in matches:\n",
    "        if m[0].distance < threshold * m[1].distance:\n",
    "            filtered_matches.append(m[0])\n",
    "    return filtered_matches\n",
    "\n",
    "def filter_ED(matches, keypoints, distance=FILTER_DISTANCE):\n",
    "    filtered_matches = []\n",
    "    for m in matches:\n",
    "        euclidean_distance = np.linalg.norm(np.array(keypoints[0][m.queryIdx].pt) - np.array(keypoints[1][m.trainIdx].pt))\n",
    "        if euclidean_distance < distance:\n",
    "            filtered_matches.append(m)\n",
    "    return filtered_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funzione di allineamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_SIFT = 0\n",
    "MODE_ORB = 1\n",
    "\n",
    "NFEATURES = 10000\n",
    "\n",
    "def align(image_1, image_2, image_to_align=None, mode=MODE_SIFT, lr=False, ed=False, nfeatures=NFEATURES):\n",
    "    \"\"\"\n",
    "    Align two images using SIFT or ORB features.\n",
    "    Can use Lowe's ratio test and Euclidean distance filtering.\n",
    "    \"\"\"\n",
    "    # If no image to align is provided, use image_2\n",
    "    if image_to_align is None:\n",
    "        image_to_align = image_2\n",
    "    \n",
    "    #Â Detect keypoints and compute descriptors using SIFT or ORB\n",
    "    if mode == MODE_SIFT:\n",
    "        sift = cv2.SIFT_create(nfeatures=nfeatures)\n",
    "        keypoints_1, descriptors_1 = sift.detectAndCompute(image_1, None)\n",
    "        keypoints_2, descriptors_2 = sift.detectAndCompute(image_2, None)\n",
    "    elif mode == MODE_ORB:\n",
    "        orb = cv2.ORB_create(nfeatures=nfeatures)\n",
    "        keypoints_1, descriptors_1 = orb.detectAndCompute(image_1, None)\n",
    "        keypoints_2, descriptors_2 = orb.detectAndCompute(image_2, None)\n",
    "    else:\n",
    "        raise Exception(\"Mode not supported\")\n",
    "    \n",
    "    # Match descriptors using bruteforce\n",
    "    if lr: # If using Lowe's ratio\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "        matches = bf.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "        matches = filter_LR(matches)\n",
    "    else: # If not using Lowe's ratio\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "        matches = bf.match(descriptors_1, descriptors_2)\n",
    "\n",
    "    if ed: # If using Euclidean distance\n",
    "        filtered_matches = filter_ED(matches, (keypoints_1, keypoints_2))\n",
    "    else:\n",
    "        filtered_matches = matches\n",
    "    \n",
    "    # Extract points from matches for RANSAC\n",
    "    points = ([], [])\n",
    "    for match in filtered_matches:\n",
    "        points[0].append(keypoints_1[match.queryIdx].pt)\n",
    "        points[1].append(keypoints_2[match.trainIdx].pt)\n",
    "    \n",
    "    points_1 = np.array(points[0])\n",
    "    points_2 = np.array(points[1])\n",
    "    warp_matrix, mask = cv2.estimateAffine2D(points_2, points_1)\n",
    "\n",
    "    # Apply the warp matrix to the image to align\n",
    "    height, width = image_to_align.shape[:2]\n",
    "    aligned_image = cv2.warpAffine(image_to_align, warp_matrix, (width, height))\n",
    "\n",
    "    return aligned_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confronti CLAHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controllo dei diversi parametri per applicare CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipLimits = [1.0, 2.0, 4.0, 8.0, 12.0, 16.0, 17.0, 18.0, 19.0, 20.0]\n",
    "tileGridSizes = [(4, 4), (7, 7), (8, 8), (9, 9), (12, 12)]\n",
    "\n",
    "label_free = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/non_colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "stained = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "highest = (None, None, 0)\n",
    "for clipLimit in clipLimits:\n",
    "    for tileGridSize in tileGridSizes:\n",
    "        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "        label_free_clahe = clahe.apply(label_free)\n",
    "        stained_clahe = clahe.apply(stained)\n",
    "\n",
    "        aligned_image = align(label_free_clahe, stained_clahe, image_to_align=stained, mode=MODE_ORB, lr=False, ed=True)\n",
    "        mi = mutual_information(label_free, aligned_image)\n",
    "\n",
    "        #print(f\"CLAHE\\tCL: {clipLimit}\\tTGS: {tileGridSize}\\tSCORE: {score}\")\n",
    "        if mi > highest[2]:\n",
    "            highest = (clipLimit, tileGridSize, mi)\n",
    "            \n",
    "print(f\"BEST - CL: {highest[0]} - TGS: {highest[1]} - SCORE: {highest[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne risulta che il metodo CLAHE migliora si ottiene con `clipLimit=18.0` e `tileGridSize=(8, 8)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizzazione delle immagini con CLAHE 18.0 (8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_free = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/non_colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "stained = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=18.0, tileGridSize=(8, 8))\n",
    "label_free_clahe = clahe.apply(label_free)\n",
    "stained_clahe = clahe.apply(stained)\n",
    "\n",
    "aligned_image = align(label_free_clahe, stained_clahe, mode=MODE_ORB, lr=False, ed=True)\n",
    "\n",
    "difference_original = cv2.absdiff(label_free_clahe, stained_clahe)\n",
    "difference_aligned_clahe = cv2.absdiff(label_free_clahe, aligned_image)\n",
    "\n",
    "show_horizontal_images([(label_free_clahe, \"Non colorata CLAHE\"), (stained_clahe, \"Colorata CLAHE\"), (aligned_image, \"Allineata CLAHE\")])\n",
    "show_horizontal_images([(difference_original, \"Differenza originale\"), (difference_aligned_clahe, \"Differenza allineata\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confronti tra SIFT e ORB con filtri LR e DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controllo con i diversi metodi e le combinazioni di filtri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usano le immagini CLAHE con i parametri prestabiliti precedentemente: `18.0 (8, 8)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_free = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/non_colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "stained = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=18.0, tileGridSize=(8, 8))\n",
    "label_free_clahe = clahe.apply(label_free)\n",
    "stained_clahe = clahe.apply(stained)\n",
    "\n",
    "highest = (None, None, None, 0)\n",
    "for mode in [MODE_SIFT, MODE_ORB]:\n",
    "    for lr in [False, True]:\n",
    "        for ed in [False, True]:\n",
    "            aligned_image = align(label_free_clahe, stained_clahe, image_to_align=stained, mode=mode, lr=lr, ed=ed)\n",
    "            mi = mutual_information(label_free, aligned_image)\n",
    "\n",
    "            #print(f\"ALIGN\\tMODE: {mode}\\tLR: {lr}\\tED: {ed}\\tSCORE: {score}\")\n",
    "            if mi > highest[3]:\n",
    "                highest = (mode, lr, ed, mi)\n",
    "\n",
    "print(f\"BEST - MODE: {'SIFT' if highest[0] == 0 else 'ORB'} - LR: {highest[1]} - ED: {highest[2]} - SCORE: {highest[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne risulta che il metodo migliore con CLAHE 18.0 (8, 8) Ã¨ SIFT senza Lowe's Ratio e con Distanza Euclidea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizzazione dell'allineamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_free = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/non_colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "stained = cv2.cvtColor(cv2.imread(\"../../Materiale/Prove/colorato.png\"), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=18.0, tileGridSize=(8, 8))\n",
    "label_free_clahe = clahe.apply(label_free)\n",
    "stained_clahe = clahe.apply(stained)\n",
    "\n",
    "#Â Detect keypoints and compute descriptors using SIFT\n",
    "sift = cv2.SIFT_create(nfeatures=10000)\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(label_free_clahe, None)\n",
    "keypoints_2, descriptors_2 = sift.detectAndCompute(stained_clahe, None)\n",
    "\n",
    "# Match descriptors using bruteforce not using Lowe's Ratio\n",
    "if False:\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(descriptors_1, descriptors_2)\n",
    "else:\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    matches = bf.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "    matches = filter_LR(matches)\n",
    "\n",
    "# Filter using Euclidean Distance\n",
    "filtered_matches = filter_ED(matches, (keypoints_1, keypoints_2), distance=200)\n",
    "\n",
    "# Extract points from matches for RANSAC\n",
    "points = ([], [])\n",
    "for match in filtered_matches:\n",
    "    points[0].append(keypoints_1[match.queryIdx].pt)\n",
    "    points[1].append(keypoints_2[match.trainIdx].pt)\n",
    "\n",
    "points_1 = np.array(points[0])\n",
    "points_2 = np.array(points[1])\n",
    "warp_matrix, mask = cv2.estimateAffine2D(points_2, points_1)\n",
    "\n",
    "# Apply the warp matrix to the image to align\n",
    "height, width = stained.shape[:2]\n",
    "aligned_image = cv2.warpAffine(stained, warp_matrix, (width, height))\n",
    "aligned_clahe = clahe.apply(aligned_image)\n",
    "mi = mutual_information(label_free, aligned_image)\n",
    "mse = calculate_mse(label_free_clahe, aligned_clahe)\n",
    "\n",
    "difference_original = cv2.absdiff(label_free_clahe, stained_clahe)\n",
    "difference_aligned = cv2.absdiff(label_free, aligned_image)\n",
    "difference_aligned_clahe = cv2.absdiff(label_free_clahe, aligned_clahe)\n",
    "\n",
    "draw_matches = cv2.drawMatches(\n",
    "    label_free_clahe, keypoints_1,\n",
    "    stained_clahe, keypoints_2,\n",
    "    filtered_matches, None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS, matchesThickness=5)\n",
    "\n",
    "original_mi = mutual_information(label_free, stained)\n",
    "original_mse = calculate_mse(label_free_clahe, stained_clahe)\n",
    "print(f\"ORIGINAL - MI: {original_mi} - MSE: {original_mse}\")\n",
    "print(f\"ALIGNED  - MI: {mi} - MSE: {mse}\")\n",
    "\n",
    "print(f\"Initial matches: {len(matches)} - Final matches: {len(filtered_matches)}\")\n",
    "show_image(draw_matches, \"Matches\")\n",
    "\n",
    "show_horizontal_images([(label_free_clahe, \"Non colorata CLAHE\"), (stained_clahe, \"Colorata CLAHE\"), (aligned_clahe, \"Allineata CLAHE\")])\n",
    "show_horizontal_images([(difference_original, \"Differenza originale CLAHE\"), (difference_aligned_clahe, \"Differenza allineata CLAHE\")])\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.hist(difference_original.ravel(), bins=256, range=(0, 256), color='black', alpha=0.75)\n",
    "plt.hist(difference_aligned_clahe.ravel(), bins=256, range=(0, 256), color='blue', alpha=0.75)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualStaining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
